{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from vit_sae_analysis.dashboard_fns import get_feature_data\n",
    "from sae_training.utils import ViTSparseAutoencoderSessionloader\n",
    "from loader import ThumbnailDataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_img_indices = torch.load('/home/ubuntu/sae_vit/dashboard/max_activating_image_indices.pt')\n",
    "mean_acts = torch.load('/home/ubuntu/sae_vit/dashboard/sae_mean_acts.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([65536, 20])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_img_indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir = 'cruft/cruft/video/'\n",
    "files = [parent_dir + f for f in os.listdir(parent_dir) if f.endswith('.h5')]\n",
    "\n",
    "dataset = ThumbnailDataset(files, keys=['likeCount', 'viewCount', 'commentCount'], device='cpu')\n",
    "max_file = 524_288\n",
    "# max_file = 100_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 104/105 [40:11<00:23, 23.18s/it]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (4288) must match the existing size (5000) at non-singleton dimension 0.  Target sizes: [4288].  Tensor sizes: [5000]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, max_file, bs)):\n\u001b[1;32m      6\u001b[0m     data \u001b[38;5;241m=\u001b[39m dataset[i:i\u001b[38;5;241m+\u001b[39mbs]\n\u001b[0;32m----> 8\u001b[0m     \u001b[43mlike_counts\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mbs\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlikeCount\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      9\u001b[0m     view_counts[i:i\u001b[38;5;241m+\u001b[39mbs] \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mviewCount\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     10\u001b[0m     comment_counts[i:i\u001b[38;5;241m+\u001b[39mbs] \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcommentCount\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (4288) must match the existing size (5000) at non-singleton dimension 0.  Target sizes: [4288].  Tensor sizes: [5000]"
     ]
    }
   ],
   "source": [
    "bs = 5000\n",
    "like_counts = torch.empty(max_file, dtype=torch.float32, device='cpu')\n",
    "view_counts = torch.empty(max_file, dtype=torch.float32, device='cpu')\n",
    "comment_counts = torch.empty(max_file, dtype=torch.float32, device='cpu')\n",
    "for i in tqdm(range(0, max_file, bs)):\n",
    "    data = dataset[i:i+bs]\n",
    "\n",
    "    like_counts[i:i+bs] = data['likeCount']\n",
    "    view_counts[i:i+bs] = data['viewCount']\n",
    "    comment_counts[i:i+bs] = data['commentCount']\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset[i:max_file]\n",
    "\n",
    "like_counts[i:max_file] = data['likeCount']\n",
    "view_counts[i:max_file] = data['viewCount']\n",
    "comment_counts[i:max_file] = data['commentCount']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_counts = view_counts.to('cuda')\n",
    "like_counts = like_counts.to('cuda')\n",
    "comment_counts = comment_counts.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_img_indices = max_img_indices.to(torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_img_likes = like_counts[max_img_indices]\n",
    "max_img_img_views = view_counts[max_img_indices]\n",
    "max_img_comment_counts = comment_counts[max_img_indices]\n",
    "\n",
    "miv_mean = max_img_likes.mean(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([5.6662e+04, 4.1990e+03, 1.8890e+03, 1.0620e+03, 6.0900e+02,\n",
       "        3.6700e+02, 1.6000e+02, 9.5000e+01, 8.5000e+01, 3.9000e+01,\n",
       "        3.2000e+01, 2.6000e+01, 2.2000e+01, 2.5000e+01, 1.1000e+01,\n",
       "        1.2000e+01, 8.0000e+00, 5.0000e+00, 1.2000e+01, 5.8000e+01,\n",
       "        7.0000e+00, 5.0000e+00, 8.0000e+00, 3.1000e+01, 1.1000e+01,\n",
       "        5.0000e+00, 1.0000e+00, 1.0000e+00, 1.8000e+01, 1.0000e+01,\n",
       "        7.0000e+00, 1.1000e+01, 6.0000e+00, 1.0000e+01, 1.0000e+00,\n",
       "        0.0000e+00, 2.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00,\n",
       "        5.0000e+00, 2.0000e+00, 1.0000e+00, 1.0000e+00, 3.0000e+00,\n",
       "        0.0000e+00, 2.0000e+00, 0.0000e+00, 2.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00]),\n",
       " array([5.59999990e+00, 8.40241028e+02, 1.67488208e+03, 2.50952319e+03,\n",
       "        3.34416406e+03, 4.17880518e+03, 5.01344629e+03, 5.84808740e+03,\n",
       "        6.68272803e+03, 7.51736914e+03, 8.35201074e+03, 9.18665137e+03,\n",
       "        1.00212920e+04, 1.08559336e+04, 1.16905742e+04, 1.25252158e+04,\n",
       "        1.33598564e+04, 1.41944971e+04, 1.50291387e+04, 1.58637793e+04,\n",
       "        1.66984199e+04, 1.75330625e+04, 1.83677031e+04, 1.92023438e+04,\n",
       "        2.00369844e+04, 2.08716250e+04, 2.17062676e+04, 2.25409082e+04,\n",
       "        2.33755488e+04, 2.42101895e+04, 2.50448301e+04, 2.58794727e+04,\n",
       "        2.67141133e+04, 2.75487539e+04, 2.83833945e+04, 2.92180352e+04,\n",
       "        3.00526777e+04, 3.08873184e+04, 3.17219590e+04, 3.25565996e+04,\n",
       "        3.33912422e+04, 3.42258828e+04, 3.50605234e+04, 3.58951641e+04,\n",
       "        3.67298047e+04, 3.75644453e+04, 3.83990859e+04, 3.92337266e+04,\n",
       "        4.00683711e+04, 4.09030117e+04, 4.17376523e+04, 4.25722930e+04,\n",
       "        4.34069336e+04, 4.42415742e+04, 4.50762148e+04, 4.59108555e+04,\n",
       "        4.67454961e+04, 4.75801406e+04, 4.84147812e+04, 4.92494219e+04,\n",
       "        5.00840625e+04, 5.09187031e+04, 5.17533438e+04, 5.25879844e+04,\n",
       "        5.34226250e+04, 5.42572656e+04, 5.50919062e+04, 5.59265508e+04,\n",
       "        5.67611914e+04, 5.75958320e+04, 5.84304727e+04, 5.92651133e+04,\n",
       "        6.00997539e+04, 6.09343945e+04, 6.17690352e+04, 6.26036758e+04,\n",
       "        6.34383203e+04, 6.42729609e+04, 6.51076016e+04, 6.59422422e+04,\n",
       "        6.67768828e+04, 6.76115234e+04, 6.84461641e+04, 6.92808047e+04,\n",
       "        7.01154453e+04, 7.09500859e+04, 7.17847266e+04, 7.26193672e+04,\n",
       "        7.34540078e+04, 7.42886484e+04, 7.51232891e+04, 7.59579375e+04,\n",
       "        7.67925781e+04, 7.76272188e+04, 7.84618594e+04, 7.92965000e+04,\n",
       "        8.01311406e+04, 8.09657812e+04, 8.18004219e+04, 8.26350625e+04,\n",
       "        8.34697031e+04]),\n",
       " <BarContainer object of 100 artists>)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(miv_mean.cpu().numpy(), bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "values, indices = torch.topk(miv_mean[:1000], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([24045.8008, 19493.9512, 16136.6504, 15969.3506, 15920.5000, 15889.3506,\n",
       "        14544.2998, 10813.7500,  8263.3506,  6846.1001], device='cuda:0')"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([809, 700, 719, 837, 655, 178, 188, 571, 398, 238], device='cuda:0')"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from vit_sae_analysis.dashboard_fns import get_feature_data\n",
    "from sae_training.utils import ViTSparseAutoencoderSessionloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDataDataset(Dataset):\n",
    "    def __init__(self, file_paths, device, keys=None):\n",
    "        self.file_paths = file_paths  # List of HDF5 file paths\n",
    "        self.index_map = []\n",
    "        self.start_end = {}\n",
    "        for file_idx, file_path in enumerate(self.file_paths):\n",
    "            initial_idx = len(self.index_map)\n",
    "            with h5py.File(file_path, 'r') as hf:\n",
    "                keys = list(hf.keys())\n",
    "                # Assuming all datasets have the same length for simplicity\n",
    "                num_samples = len(hf[keys[0]])\n",
    "                for i in range(num_samples):\n",
    "                    self.index_map.append((file_idx, i))\n",
    "\n",
    "            self.start_end[file_idx] = (initial_idx, len(self.index_map))\n",
    "                \n",
    "        self.device = device\n",
    "        self.keys = keys\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.index_map)\n",
    "\n",
    "    def _get_item(self, idx, keys):\n",
    "        return self._get_file_slice(idx, idx + 1, keys)\n",
    "\n",
    "    def _get_file_slice(self, absolute_start, absolute_end, wanted_keys=None):\n",
    "        file_idx, _ = self.index_map[absolute_start]\n",
    "        file_start, _ = self.start_end[file_idx]\n",
    "\n",
    "\n",
    "        relative_start = absolute_start - file_start\n",
    "        relative_end = absolute_end - file_start\n",
    "        file_path = self.file_paths[file_idx]\n",
    "\n",
    "        with h5py.File(file_path, 'r') as hf:\n",
    "            keys = list(hf.keys())\n",
    "            if wanted_keys is not None:\n",
    "                keys = [key for key in keys if key in wanted_keys]\n",
    "            data = {\n",
    "                'thumbnailStandard': torch.tensor(hf['thumbnailStandard'][relative_start:relative_end], device=self.device),\n",
    "                'likeCount': torch.tensor(hf['likeCount'][relative_start:relative_end], device=self.device) if 'likeCount' in keys else None,\n",
    "                'viewCount': torch.tensor(hf['viewCount'][relative_start:relative_end], device=self.device) if 'viewCount' in keys else None,\n",
    "                'commentCount': torch.tensor(hf['commentCount'][relative_start:relative_end], device=self.device) if 'commentCount' in keys else None,\n",
    "                'title': list(hf['title'][relative_start:relative_end]) if 'title' in keys else None\n",
    "            }\n",
    "        return data\n",
    "\n",
    "    def _get_whole_file(self, file_idx, keys):\n",
    "        start, end = self.start_end[file_idx]\n",
    "        return self._get_file_slice(start, end, keys)\n",
    "\n",
    "    def _return_multi_file_slice(self, start, end, keys):\n",
    "        all_indices = self.index_map[start:end]\n",
    "\n",
    "        whole_files = list()\n",
    "\n",
    "        last_file_idx = None\n",
    "        first_file_idx = None\n",
    "        for file_idx, _ in all_indices:\n",
    "            if file_idx not in whole_files:\n",
    "                whole_files.append(file_idx)\n",
    "            last_file_idx = file_idx\n",
    "            \n",
    "            if first_file_idx is None:\n",
    "                first_file_idx = file_idx\n",
    "\n",
    "        if first_file_idx == last_file_idx:\n",
    "            return self._get_file_slice(start, end, keys)\n",
    "\n",
    "        whole_files = [idx for idx in whole_files if idx != first_file_idx and idx != last_file_idx]\n",
    "\n",
    "        sample_lists = []\n",
    "    \n",
    "        _, end_of_first_file = self.start_end[first_file_idx]\n",
    "        first_file_samples = self._get_file_slice(start, end_of_first_file, keys)\n",
    "        sample_lists.append(first_file_samples)\n",
    "\n",
    "        for file_idx in whole_files:\n",
    "            sample_lists.append(self._get_whole_file(file_idx, keys))\n",
    "\n",
    "        start_of_last_file, _ = self.start_end[last_file_idx]\n",
    "        last_file_samples = self._get_file_slice(start_of_last_file, end, keys)\n",
    "        sample_lists.append(last_file_samples)\n",
    "        \n",
    "        data = None\n",
    "        for samples in sample_lists:\n",
    "            if data is None:\n",
    "                data = samples\n",
    "            else:\n",
    "                for key in data.keys():\n",
    "                    if key == 'title':\n",
    "                        data[key] = data[key] + samples[key]\n",
    "                    else:    \n",
    "                        data[key] = torch.cat((data[key], samples[key]), 0)\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if isinstance(idx, slice):\n",
    "            start = idx.start if idx.start is not None else 0\n",
    "            stop = idx.stop if idx.stop is not None else len(self.index_map)\n",
    "\n",
    "            if idx.step is not None:\n",
    "                raise ValueError(\"Slicing with step is not supported\", idx)\n",
    "\n",
    "            return self._return_multi_file_slice(start, stop, self.keys)\n",
    "\n",
    "        return self._get_item(idx, self.keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir = 'cruft/cruft/video/'\n",
    "files = [parent_dir + f for f in os.listdir(parent_dir) if f.endswith('.h5')]\n",
    "\n",
    "dataset = VideoDataDataset(files, keys=['thumbnailStandard'], device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'checkpoints/4rfb746w/final_sparse_autoencoder_openai/clip-vit-large-patch14_-2_resid_65536.pt'\n",
    "data = torch.load(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = data['cfg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c82385b523ff4bdfa9aa847de6d41cf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/52 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "520f334b8b1048e79cc8abf795fd0f0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/52 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d6a4e620e7d49c3907c66f7a8c08662",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to create the data loader!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filling activation store with images: 100%|██████████| 15360/15360 [00:13<00:00, 1176.75it/s]\n",
      "Getting batches for SAE: 100%|██████████| 15/15 [02:06<00:00,  8.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loader created!!!\n"
     ]
    }
   ],
   "source": [
    "model, sparse_autoencoder, activations_loader = ViTSparseAutoencoderSessionloader.load_session_from_pretrained(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sae_training.hooked_vit.HookedVisionTransformer at 0x7f4474275150>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_batch = dataset[0:5]['thumbnailStandard']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPProcessor:\n",
       "- image_processor: CLIPImageProcessor {\n",
       "  \"crop_size\": {\n",
       "    \"height\": 224,\n",
       "    \"width\": 224\n",
       "  },\n",
       "  \"do_center_crop\": true,\n",
       "  \"do_convert_rgb\": true,\n",
       "  \"do_normalize\": true,\n",
       "  \"do_rescale\": true,\n",
       "  \"do_resize\": true,\n",
       "  \"feature_extractor_type\": \"CLIPFeatureExtractor\",\n",
       "  \"image_mean\": [\n",
       "    0.48145466,\n",
       "    0.4578275,\n",
       "    0.40821073\n",
       "  ],\n",
       "  \"image_processor_type\": \"CLIPImageProcessor\",\n",
       "  \"image_std\": [\n",
       "    0.26862954,\n",
       "    0.26130258,\n",
       "    0.27577711\n",
       "  ],\n",
       "  \"resample\": 3,\n",
       "  \"rescale_factor\": 0.00392156862745098,\n",
       "  \"size\": {\n",
       "    \"shortest_edge\": 224\n",
       "  },\n",
       "  \"use_square_size\": false\n",
       "}\n",
       "\n",
       "- tokenizer: CLIPTokenizerFast(name_or_path='openai/clip-vit-large-patch14', vocab_size=49408, model_max_length=77, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|startoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t49406: AddedToken(\"<|startoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t49407: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = model.processor(images=image_batch, text = \"\", return_tensors=\"pt\", padding = True).to(model.model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 224, 224])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.pixel_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[49406, 49407]], device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'pixel_values'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e12340f43f76470ab75d0bd8640fdd66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/52 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a98f4536f8934f4f89181c7913c912f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/52 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74991945ffd74c6ea6b8f696d98745ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:07<00:00, 129.52it/s]\n"
     ]
    }
   ],
   "source": [
    "get_feature_data(\n",
    "    sparse_autoencoder,\n",
    "    model,\n",
    "    load_pretrained=True,\n",
    "    threshold=0.04,\n",
    "    number_of_images=524_288,\n",
    "    number_of_max_activating_images=20,\n",
    "    dataset=dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
